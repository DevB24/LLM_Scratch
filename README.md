**LLM from Scratch**

This repository features a **Large Language Model (LLM)** built from scratch using PyTorch and Python with a focus on simplicity and efficiency.

To ensure easy training and inference on a laptop with 8GB RAM and a CPU, all hyperparameters are optimized to be lightweight while maintaining performance.

It consists the below content: </br>
-- **Tokenization**</br>
-- **Pretraining** </br>
-- **Multihead Attention**</br>
-- **Transformer Block**</br>
-- **Positional Encoding**</br>
-- **Layer Normalization** and</br>
-- **Shortcut Connection**
