{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_config = {\"vocab_size\": 50257,\n",
    "              \"drop_rate\" : 0.1,\n",
    "              \"context_length\":1024,\n",
    "              \"n_heads\": 12,\n",
    "              \"n_layers\":12,\n",
    "              \"emb_dim\":768,\n",
    "              \"qkv_bias\":False\n",
    "              \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gelu Activation function implementation for smoother Non linearities than Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Attention Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadCausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout,n_heads,qkv_bias=False):\n",
    "        super().__init__()\n",
    "            \n",
    "        assert (d_out % n_heads == 0), \\\n",
    "    \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = n_heads\n",
    "        # self.head_dim = d_out // num_heads \n",
    "        self.head_dim = d_out // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.key_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.value_w = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "       b ,num_tokens, d_in= x.shape\n",
    "       \n",
    "       query = self.query_w(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "       key = self.key_w(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "       value = self.value_w(x).view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "       key = key.transpose(1, 2)\n",
    "       query = query.transpose(1, 2)\n",
    "       value = value.transpose(1, 2)\n",
    "       \n",
    "       \n",
    "       attn_scores = query @ key.transpose(2, 3)  # Dot product for each head\n",
    "       \n",
    "       \n",
    "       mask_bool = self.mask[:num_tokens, :num_tokens].bool()\n",
    "       attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "       attent_weights = torch.softmax(attn_scores/key.shape[-1] ** 0.5 , dim=-1)\n",
    "       \n",
    "       attent_weights = self.dropout(attent_weights)\n",
    "       \n",
    "       context_vec = (attent_weights @ value).transpose(1, 2) \n",
    "       \n",
    "       context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "       \n",
    "       context_vec = self.out_proj(context_vec)\n",
    "       return context_vec\n",
    "       \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding a Transformer Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 =  LayerNorm(cfg['emb_dim'])\n",
    "        self.layer_norm2 =  LayerNorm(cfg['emb_dim'])\n",
    "        self.att = MultiheadCausalAttention(d_in=cfg['emb_dim'],d_out=cfg['emb_dim'],context_length=cfg['context_length'],n_heads=cfg['n_heads'],dropout=cfg['drop_rate'],qkv_bias=cfg['qkv_bias'])\n",
    "        \n",
    "        self.ff = Feedforward(cfg)\n",
    "        self.shortcut_drop = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x= self.att(x)\n",
    "        x = self.shortcut_drop(x)\n",
    "        x = x+ shortcut\n",
    "        \n",
    "        # The above x = x+ shortcut  is for adding the values from the input to the output of the dropout layer to avoid the vanishing Gradient problem during the back propagation.\n",
    "        \n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.shortcut_drop(x)\n",
    "        x = x+ shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the Gpt Block which will contain a 12 Layer NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_layer = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
    "        self.pos_layer = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
    "        \n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "        self.transformer_block = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg['emb_dim'],cfg['vocab_size'], bias =False)\n",
    "        \n",
    "        \n",
    "    def forward(self,x_id):\n",
    "        batch_size, seq_len = x_id.shape\n",
    "        \n",
    "        tok_emb = self.token_layer(x_id)\n",
    "        pos_emb = self.pos_layer(torch.arange(seq_len, device=x_id.device))\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'drop_rate': 0.1,\n",
       " 'context_length': 1024,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'emb_dim': 768,\n",
       " 'qkv_bias': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   72,  1842, 10850, 18252],\n",
      "        [ 3198,  2239,   379,   220]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"i love Machine Learning\"\n",
    "txt2 = \"One step at \"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[   72,  1842, 10850, 18252],\n",
      "        [ 3198,  2239,   379,   220]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.0424,  0.4464,  0.9319,  ...,  0.1077,  1.0280,  0.3325],\n",
      "         [ 0.6158,  0.2995, -0.3806,  ..., -0.2127, -0.4360,  0.5152],\n",
      "         [ 0.6728,  0.3157, -0.3430,  ...,  0.3751, -0.7065,  0.1711],\n",
      "         [-0.2579,  0.0941, -0.4393,  ...,  1.1880, -0.3813, -0.2190]],\n",
      "\n",
      "        [[ 0.0318, -0.5157,  0.1368,  ..., -0.2589, -0.5309, -0.5686],\n",
      "         [ 0.7093, -0.1975, -0.6394,  ..., -0.2053, -0.4661,  0.6504],\n",
      "         [ 0.1872, -0.1331,  0.3191,  ...,  0.1674, -0.0223, -0.1469],\n",
      "         [ 0.0863, -0.4963, -0.0773,  ...,  0.4304, -1.1178,  0.1237]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GptModel(GPT_config)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum([p.numel() for p in model.parameters() if p.requires_grad==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124412160"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total - sum(p.numel() for p in model.out_head.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch._VariableFunctionsClass.empty>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
